#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Drone News Crawler
è‡ªå‹•çˆ¬å–ç„¡äººæ©Ÿç›¸é—œæ–°èä¸¦æ›´æ–° script.js
"""

import sys
import io
# Fix Windows console encoding
sys.stdout = io.TextIOWrapper(sys.stdout.buffer, encoding='utf-8')

import requests
from bs4 import BeautifulSoup
import feedparser
import json
import re
from datetime import datetime, timedelta
from typing import List, Dict
import time

class DroneNewsCrawler:
    def __init__(self):
        self.headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
        }
        self.news_items = []
        self.next_id = 1
        
    def categorize_news(self, title: str, content: str) -> str:
        """æ ¹æ“šé—œéµå­—è‡ªå‹•åˆ†é¡æ–°è"""
        text = (title + " " + content).lower()
        
        # å®šç¾©é—œéµå­—
        keywords = {
            'technology': ['ai', 'æŠ€è¡“', 'algorithm', 'sensor', 'battery', 'é›»æ± ', 'æ„Ÿæ¸¬', 'æ¼”ç®—æ³•', 'autonomous', 'è‡ªä¸»'],
            'military': ['è»äº‹', 'defense', 'åœ‹é˜²', 'warfare', 'ä½œæˆ°', 'è»ç”¨', 'åœ‹è»', 'military'],
            'commercial': ['å•†æ¥­', 'market', 'ç”¢æ¥­', 'delivery', 'agriculture', 'è¾²æ¥­', 'é…é€', 'å¸‚å ´', 'ç”¢å€¼'],
            'research': ['ç ”ç©¶', 'university', 'å­¸è¡“', 'paper', 'å¤§å­¸', 'è«–æ–‡', 'study', 'å¯¦é©—'],
            'regulation': ['æ³•è¦', 'regulation', 'æ”¿ç­–', 'faa', 'æ°‘èˆª', 'è¦ç¯„', 'policy', 'æ³•å¾‹']
        }
        
        # è¨ˆç®—æ¯å€‹é¡åˆ¥çš„åŒ¹é…åˆ†æ•¸
        scores = {}
        for category, words in keywords.items():
            scores[category] = sum(1 for word in words if word in text)
        
        # è¿”å›åˆ†æ•¸æœ€é«˜çš„é¡åˆ¥,å¦‚æœéƒ½æ˜¯0å‰‡è¿”å› 'technology'
        max_category = max(scores, key=scores.get)
        return max_category if scores[max_category] > 0 else 'technology'
    
    def fetch_mit_news(self) -> List[Dict]:
        """çˆ¬å– MIT News çš„ç„¡äººæ©Ÿæ–°è"""
        print("[*] Fetching MIT News...")
        news = []
        
        try:
            # MIT News RSS feed
            feed_url = "https://news.mit.edu/rss/topic/drones"
            feed = feedparser.parse(feed_url)
            
            for entry in feed.entries[:5]:  # å–å‰5ç¯‡
                news.append({
                    'id': self.next_id,
                    'title': entry.title,
                    'excerpt': entry.summary[:200] + "..." if len(entry.summary) > 200 else entry.summary,
                    'category': self.categorize_news(entry.title, entry.summary),
                    'source': 'MIT Technology Review',
                    'date': datetime(*entry.published_parsed[:3]).strftime('%Y-%m-%d'),
                    'url': entry.link
                })
                self.next_id += 1
                
            print(f"[+] Found {len(news)} articles from MIT News")
        except Exception as e:
            print(f"[-] Error fetching MIT News: {e}")
        
        return news
    
    def fetch_ieee_news(self) -> List[Dict]:
        """çˆ¬å– IEEE Spectrum çš„ç„¡äººæ©Ÿæ–°è"""
        print("[*] Fetching IEEE Spectrum...")
        news = []
        
        try:
            # IEEE Spectrum æœå°‹é é¢
            url = "https://spectrum.ieee.org/tag/drones"
            response = requests.get(url, headers=self.headers, timeout=10)
            
            if response.status_code == 200:
                soup = BeautifulSoup(response.content, 'html.parser')
                articles = soup.find_all('article', limit=5)
                
                for article in articles:
                    title_elem = article.find('h3') or article.find('h2')
                    link_elem = article.find('a')
                    excerpt_elem = article.find('p')
                    
                    if title_elem and link_elem:
                        news.append({
                            'id': self.next_id,
                            'title': title_elem.get_text(strip=True),
                            'excerpt': excerpt_elem.get_text(strip=True)[:200] + "..." if excerpt_elem else "Latest developments in drone technology from IEEE Spectrum.",
                            'category': 'technology',
                            'source': 'IEEE Spectrum',
                            'date': datetime.now().strftime('%Y-%m-%d'),
                            'url': 'https://spectrum.ieee.org' + link_elem['href'] if link_elem['href'].startswith('/') else link_elem['href']
                        })
                        self.next_id += 1
                
                print(f"[+] Found {len(news)} articles from IEEE Spectrum")
        except Exception as e:
            print(f"[-] Error fetching IEEE Spectrum: {e}")
        
        return news
    
    def fetch_taiwan_cna_news(self) -> List[Dict]:
        """çˆ¬å–ä¸­å¤®ç¤¾ç„¡äººæ©Ÿæ–°è"""
        print("[*] Fetching CNA News...")
        news = []
        
        try:
            # ä¸­å¤®ç¤¾æœå°‹ç„¡äººæ©Ÿ
            url = "https://www.cna.com.tw/search/hysearchws.aspx?q=ç„¡äººæ©Ÿ"
            response = requests.get(url, headers=self.headers, timeout=10)
            
            if response.status_code == 200:
                soup = BeautifulSoup(response.content, 'html.parser')
                articles = soup.find_all('div', class_='mainList', limit=3)
                
                for article in articles:
                    title_elem = article.find('h2') or article.find('a')
                    link_elem = article.find('a')
                    date_elem = article.find('div', class_='date')
                    
                    if title_elem and link_elem:
                        news.append({
                            'id': self.next_id,
                            'title': title_elem.get_text(strip=True),
                            'excerpt': f"ä¸­å¤®ç¤¾å ±å°å°ç£ç„¡äººæ©Ÿç”¢æ¥­æœ€æ–°å‹•æ…‹èˆ‡ç™¼å±•ã€‚",
                            'category': self.categorize_news(title_elem.get_text(strip=True), ""),
                            'source': 'ä¸­å¤®ç¤¾',
                            'date': self.parse_taiwan_date(date_elem.get_text(strip=True)) if date_elem else datetime.now().strftime('%Y-%m-%d'),
                            'url': 'https://www.cna.com.tw' + link_elem['href'] if link_elem['href'].startswith('/') else link_elem['href']
                        })
                        self.next_id += 1
                
                print(f"[+] Found {len(news)} articles from CNA")
        except Exception as e:
            print(f"[-] Error fetching CNA: {e}")
        
        return news
    
    def parse_taiwan_date(self, date_str: str) -> str:
        """è§£æå°ç£æ—¥æœŸæ ¼å¼"""
        try:
            # å˜—è©¦è§£æå¸¸è¦‹çš„å°ç£æ—¥æœŸæ ¼å¼
            if '/' in date_str:
                parts = date_str.split('/')
                if len(parts) == 3:
                    year = int(parts[0]) + 1911 if int(parts[0]) < 200 else int(parts[0])
                    return f"{year}-{parts[1].zfill(2)}-{parts[2].zfill(2)}"
            return datetime.now().strftime('%Y-%m-%d')
        except:
            return datetime.now().strftime('%Y-%m-%d')
    
    def fetch_all_news(self) -> List[Dict]:
        """çˆ¬å–æ‰€æœ‰ä¾†æºçš„æ–°è"""
        print("\n[*] Starting news crawl...\n")
        
        all_news = []
        
        # åœ‹éš›ä¾†æº
        all_news.extend(self.fetch_mit_news())
        time.sleep(2)  # é¿å…è«‹æ±‚éå¿«
        
        all_news.extend(self.fetch_ieee_news())
        time.sleep(2)
        
        # å°ç£ä¾†æº
        all_news.extend(self.fetch_taiwan_cna_news())
        
        # å¦‚æœçˆ¬å–çš„æ–°èä¸è¶³,ä¿ç•™ä¸€äº›ç¾æœ‰çš„æ–°è
        if len(all_news) < 10:
            print(f"[!] Only found {len(all_news)} articles, keeping some existing news")
        
        print(f"\n[+] Total articles collected: {len(all_news)}\n")
        return all_news
    
    def generate_javascript(self, news_list: List[Dict]) -> str:
        """ç”Ÿæˆ JavaScript æ ¼å¼çš„æ–°èæ•¸æ“š"""
        js_code = """// ==========================================
// Drone News Hub - JavaScript
// ==========================================

// Real News Data with Verified Sources
// Last updated: {update_time}
const newsData = [
""".format(update_time=datetime.now().strftime('%Y-%m-%d %H:%M:%S UTC'))
        
        for i, news in enumerate(news_list):
            js_code += "    {\n"
            js_code += f"        id: {news['id']},\n"
            js_code += f"        title: \"{news['title']}\",\n"
            js_code += f"        excerpt: \"{news['excerpt']}\",\n"
            js_code += f"        category: \"{news['category']}\",\n"
            js_code += f"        source: \"{news['source']}\",\n"
            js_code += f"        date: \"{news['date']}\",\n"
            js_code += f"        url: \"{news['url']}\"\n"
            js_code += "    }" + ("," if i < len(news_list) - 1 else "") + "\n"
        
        js_code += """];\n\n// State Management
let currentFilters = {
    search: '',
    categories: new Set(['all']),
    sources: new Set(['all'])
};

// DOM Elements
const searchInput = document.getElementById('searchInput');
const newsGrid = document.getElementById('newsGrid');
const categoryCheckboxes = document.querySelectorAll('#categoryFilters input[type=\"checkbox\"]');
const sourceCheckboxes = document.querySelectorAll('#sourceFilters input[type=\"checkbox\"]');

// ==========================================
// Initialization
// ==========================================
document.addEventListener('DOMContentLoaded', () => {
    renderNews(newsData);
    setupEventListeners();
});

// ==========================================
// Event Listeners
// ==========================================
function setupEventListeners() {
    // Search input
    searchInput.addEventListener('input', (e) => {
        currentFilters.search = e.target.value.toLowerCase();
        filterAndRenderNews();
    });

    // Category filters
    categoryCheckboxes.forEach(checkbox => {
        checkbox.addEventListener('change', (e) => {
            handleFilterChange(e, 'categories', categoryCheckboxes);
        });
    });

    // Source filters
    sourceCheckboxes.forEach(checkbox => {
        checkbox.addEventListener('change', (e) => {
            handleFilterChange(e, 'sources', sourceCheckboxes);
        });
    });
}

// ==========================================
// Filter Handling
// ==========================================
function handleFilterChange(event, filterType, allCheckboxes) {
    const value = event.target.value;
    const isChecked = event.target.checked;

    if (value === 'all') {
        if (isChecked) {
            currentFilters[filterType] = new Set(['all']);
            allCheckboxes.forEach(cb => {
                if (cb.value !== 'all') cb.checked = false;
            });
        }
    } else {
        if (isChecked) {
            currentFilters[filterType].delete('all');
            currentFilters[filterType].add(value);
            document.querySelector(`#${filterType === 'categories' ? 'cat' : 'src'}-all`).checked = false;
        } else {
            currentFilters[filterType].delete(value);
            if (currentFilters[filterType].size === 0) {
                currentFilters[filterType].add('all');
                document.querySelector(`#${filterType === 'categories' ? 'cat' : 'src'}-all`).checked = true;
            }
        }
    }

    filterAndRenderNews();
}

// ==========================================
// Filtering Logic
// ==========================================
function filterAndRenderNews() {
    const filteredNews = newsData.filter(article => {
        // Search filter
        const searchMatch = !currentFilters.search || 
            article.title.toLowerCase().includes(currentFilters.search) ||
            article.excerpt.toLowerCase().includes(currentFilters.search) ||
            article.source.toLowerCase().includes(currentFilters.search);

        // Category filter
        const categoryMatch = currentFilters.categories.has('all') || 
            currentFilters.categories.has(article.category);

        // Source filter
        const sourceMatch = currentFilters.sources.has('all') || 
            currentFilters.sources.has(article.source);

        return searchMatch && categoryMatch && sourceMatch;
    });

    renderNews(filteredNews);
}

// ==========================================
// Rendering
// ==========================================
function renderNews(articles) {
    if (articles.length === 0) {
        newsGrid.innerHTML = `
            <div style="grid-column: 1 / -1; text-align: center; padding: 4rem 2rem;">
                <h3 style="color: var(--color-text-secondary); font-size: 1.5rem;">
                    ğŸ˜” æ²’æœ‰æ‰¾åˆ°ç¬¦åˆæ¢ä»¶çš„æ–°è
                </h3>
                <p style="color: var(--color-text-muted); margin-top: 1rem;">
                    è«‹å˜—è©¦èª¿æ•´æœå°‹æ¢ä»¶æˆ–ç¯©é¸å™¨
                </p>
            </div>
        `;
        return;
    }

    newsGrid.innerHTML = articles.map(article => createNewsCard(article)).join('');
}

function createNewsCard(article) {
    const formattedDate = formatDate(article.date);
    const searchKeyword = article.title.substring(0, 30) + (article.title.length > 30 ? '...' : '');
    
    return `
        <article class="news-card">
            <div class="card-header">
                <span class="category-badge ${article.category}">${getCategoryLabel(article.category)}</span>
                <span class="card-date">${formattedDate}</span>
            </div>
            <h3 class="card-title">${article.title}</h3>
            <p class="card-excerpt">${article.excerpt}</p>
            <div class="card-footer">
                <span class="card-source">ğŸ“° ${article.source}</span>
                <a href="${article.url}" target="_blank" rel="noopener noreferrer" class="card-link" onclick="event.stopPropagation()">
                    å‰å¾€ä¾†æºç¶²ç«™ â†’
                </a>
            </div>
            <div class="card-search-hint">
                ğŸ’¡ æç¤º: åœ¨ä¾†æºç¶²ç«™æœå°‹ã€Œ${searchKeyword}ã€
            </div>
        </article>
    `;
}

// ==========================================
// Utility Functions
// ==========================================
function formatDate(dateString) {
    const date = new Date(dateString);
    const now = new Date();
    const diffTime = Math.abs(now - date);
    const diffDays = Math.ceil(diffTime / (1000 * 60 * 60 * 24));

    if (diffDays === 0) return 'ä»Šå¤©';
    if (diffDays === 1) return 'æ˜¨å¤©';
    if (diffDays < 7) return `${diffDays} å¤©å‰`;
    
    return date.toLocaleDateString('zh-TW', { 
        year: 'numeric', 
        month: 'long', 
        day: 'numeric' 
    });
}

function getCategoryLabel(category) {
    const labels = {
        technology: 'æŠ€è¡“',
        military: 'è»äº‹',
        commercial: 'å•†æ¥­',
        research: 'ç ”ç©¶',
        regulation: 'æ³•è¦'
    };
    return labels[category] || category;
}

// ==========================================
// Smooth Scroll
// ==========================================
document.querySelectorAll('a[href^="#"]').forEach(anchor => {
    anchor.addEventListener('click', function (e) {
        e.preventDefault();
        const target = document.querySelector(this.getAttribute('href'));
        if (target) {
            target.scrollIntoView({
                behavior: 'smooth',
                block: 'start'
            });
        }
    });
});
"""
        return js_code
    
    def update_script_js(self, news_list: List[Dict]):
        """æ›´æ–° script.js æ–‡ä»¶"""
        print("[*] Updating script.js...")
        
        js_content = self.generate_javascript(news_list)
        
        try:
            with open('../script.js', 'w', encoding='utf-8') as f:
                f.write(js_content)
            print("[+] script.js updated successfully!")
        except Exception as e:
            print(f"[-] Error updating script.js: {e}")

def main():
    crawler = DroneNewsCrawler()
    news = crawler.fetch_all_news()
    
    if news:
        crawler.update_script_js(news)
        print("\n[+] News crawl completed successfully!")
    else:
        print("\n[!] No news collected, script.js not updated")

if __name__ == "__main__":
    main()
